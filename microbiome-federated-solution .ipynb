{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13097453,"sourceType":"datasetVersion","datasetId":8296303}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MPEG-G Microbiome Classification - Federated Solution\n\n\n**Performance Achieved:**\n- **Loss: 0.0296**\n- **Accuracy: 0.9962**\n  \n**Federated Learning**: Neural Networks with FedAvg\n\n**Approach**: Deterministic federated training\n\n**Pipeline:** kmercount Data preprocessing ‚Üí Federated client setup ‚Üí Neural Networks ‚Üí FedAvg ‚Üí Predictions\n\n**Runtime Tracking:** All execution times are captured for each step","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Configuration","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom scipy.stats import gmean\nimport warnings\nimport time\nfrom pathlib import Path\nfrom collections import defaultdict\nimport os\nimport random\n\nwarnings.filterwarnings('ignore')\n\n# COMPLETE DETERMINISTIC SETUP - MUST BE FIRST\ndef setup_deterministic_environment(seed=42):\n    \"\"\"Complete deterministic setup for reproducible results\"\"\"\n    # Python random\n    random.seed(seed)\n    \n    # NumPy random\n    np.random.seed(seed)\n    \n    # PyTorch random\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    \n    # Environment variables\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n    \n    # PyTorch deterministic operations\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    # Force deterministic algorithms\n    try:\n        torch.use_deterministic_algorithms(True)\n        print(\"‚úÖ Full deterministic mode enabled\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Partial deterministic mode: {e}\")\n\n# Apply deterministic setup IMMEDIATELY\nSEED = 42\nsetup_deterministic_environment(SEED)\n\n# Runtime tracking \nruntime_log = {}\nstart_time_total = time.time()\n\ndef log_runtime(step_name, start_time):\n    \"\"\"Log runtime for a step - compatible with microbiome_solution_notebook\"\"\"\n    elapsed = time.time() - start_time\n    runtime_log[step_name] = elapsed\n    print(f\"‚è±Ô∏è  {step_name}: {elapsed:.2f}s\")\n    return elapsed\n\nprint(\"üîß Setup with FULL DETERMINISTIC MODE\")\nprint(f\"üìÖ Pipeline started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Kaggle paths \nDATA_DIR = Path(\"/kaggle/input/microbiome-challengezindi-kmercount\") # Uploaded dataset train_kmercount.csv, test_kmercount.csv, Train.csv, Test.csv\nOUTPUT_DIR = Path(\"/kaggle/working\")\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Check files\nfiles_to_check = [\n    DATA_DIR / \"Train.csv\",\n    DATA_DIR / \"train_kmercount.csv\", \n    DATA_DIR / \"test_kmercount.csv\"\n]\n\nfor file in files_to_check:\n    if file.exists():\n        size_mb = file.stat().st_size / (1024**2)\n        print(f\"‚úÖ {file.name}: {size_mb:.1f} MB\")\n    else:\n        print(f\"‚ùå Missing: {file}\")\n\n# TEST DETERMINISM\nprint(\"\\nüß™ Testing determinism...\")\nnp.random.seed(SEED)\ntest_array1 = np.random.random(5)\nnp.random.seed(SEED)  \ntest_array2 = np.random.random(5)\nprint(f\"NumPy deterministic: {np.array_equal(test_array1, test_array2)}\")\n\ntorch.manual_seed(SEED)\ntest_tensor1 = torch.randn(3, 3)\ntorch.manual_seed(SEED)\ntest_tensor2 = torch.randn(3, 3)\nprint(f\"PyTorch deterministic: {torch.equal(test_tensor1, test_tensor2)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T14:17:18.788904Z","iopub.execute_input":"2025-09-18T14:17:18.789134Z","iopub.status.idle":"2025-09-18T14:17:24.308994Z","shell.execute_reply.started":"2025-09-18T14:17:18.789119Z","shell.execute_reply":"2025-09-18T14:17:24.308297Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Full deterministic mode enabled\nüîß Setup with FULL DETERMINISTIC MODE\nüìÖ Pipeline started at: 2025-09-18 14:17:24\n‚úÖ Train.csv: 0.1 MB\n‚úÖ train_kmercount.csv: 220.7 MB\n‚úÖ test_kmercount.csv: 80.4 MB\n\nüß™ Testing determinism...\nNumPy deterministic: True\nPyTorch deterministic: True\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# CONFIGURATION \n# =====================================================\n\n# Random seed for reproducibility \nRANDOM_STATE = 42\nSEED = 42 \n\n# COMPLETE DETERMINISTIC SETUP - MUST BE FIRST\ndef setup_deterministic_environment(seed=42):\n    \"\"\"Complete deterministic setup for reproducible results\"\"\"\n    # Python random\n    random.seed(seed)\n    \n    # NumPy random\n    np.random.seed(seed)\n    \n    # PyTorch random\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    \n    # Environment variables\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n    \n    # PyTorch deterministic operations\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    # Force deterministic algorithms\n    try:\n        torch.use_deterministic_algorithms(True)\n        print(\"‚úÖ Full deterministic mode enabled\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Partial deterministic mode: {e}\")\n\n# Apply deterministic setup IMMEDIATELY\nsetup_deterministic_environment(SEED)\n\n# ML settings \nMAX_FEATURES = 2000\nPSEUDOCOUNT = 1e-6\n\n# Neural network specific settings\nBATCH_SIZE = 64\nLR = 0.001\nEPOCHS = 20\nDROPOUT = 0.2\n\n# Device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Configuration set:\")\nprint(f\"  Random state: {RANDOM_STATE}\")\nprint(f\"  Max features: {MAX_FEATURES}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Learning rate: {LR}\")\nprint(f\"  Device: {device}\")\nprint(f\"üéØ FIXED PARAMETERS FOR REPRODUCIBILITY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T14:18:04.746040Z","iopub.execute_input":"2025-09-18T14:18:04.746442Z","iopub.status.idle":"2025-09-18T14:18:04.755619Z","shell.execute_reply.started":"2025-09-18T14:18:04.746420Z","shell.execute_reply":"2025-09-18T14:18:04.755005Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Full deterministic mode enabled\nConfiguration set:\n  Random state: 42\n  Max features: 2000\n  Batch size: 64\n  Learning rate: 0.001\n  Device: cuda\nüéØ FIXED PARAMETERS FOR REPRODUCIBILITY\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 2. Data Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"step_start = time.time()\nprint(\"üìä STEP 1: DATA PREPROCESSING PIPELINE\")\nprint(\"-\" * 40)\n\ndef clr_transform(data, pseudocount=PSEUDOCOUNT):\n    \"\"\"Apply Centered Log-Ratio transformation to compositional data\"\"\"\n    # Add pseudocount to avoid log(0)\n    data_pseudo = data + pseudocount\n    # Calculate geometric mean for each sample\n    geom_means = gmean(data_pseudo, axis=1)\n    # Apply CLR transformation\n    clr_data = np.log(data_pseudo / geom_means[:, np.newaxis])\n    return clr_data\n\ndef deterministic_mutual_info(X, y):\n    \"\"\"Deterministic mutual information with fixed random state\"\"\"\n    return mutual_info_classif(X, y, random_state=RANDOM_STATE)\n\n# Load data \nkmercount_df = pd.read_csv(DATA_DIR / \"train_kmercount.csv\")\ntrain_labels_df = pd.read_csv(DATA_DIR / \"Train.csv\")\ntest_kmercount_df = pd.read_csv(DATA_DIR / \"test_kmercount.csv\")\n\nprint(f\"K-mer counts: {kmercount_df.shape}\")\nprint(f\"Train labels: {train_labels_df.shape}\")\nprint(f\"Test k-mer counts: {test_kmercount_df.shape}\")\n\n# Fix filename mismatch - SAME AS CENTRALIZED\ntrain_labels_df['filename'] = train_labels_df['filename'].str.replace('.mgb', '')\n\n# Merge k-mer counts with sample types - SAME AS CENTRALIZED\ntrain_features_df = kmercount_df.merge(train_labels_df, on='filename', how='inner')\nprint(f\"Merged data: {train_features_df.shape}\")\n\n\nfilename_col = 'filename'\nsample_type_col = 'SampleType'\n\nprint(\"Sample distribution:\", train_features_df[sample_type_col].value_counts().to_dict())\n\n# Feature engineering \nfeature_columns = [col for col in train_features_df.columns\n                  if col not in [filename_col, sample_type_col, 'SubjectID', 'SampleID']]\n\nX_full = train_features_df[feature_columns].fillna(0)\n\n# Apply CLR transformation \nprint(\"üî¨ Applying CLR transformation...\")\nclr_start = time.time()\nX_clr = clr_transform(X_full.values)\nX_full = pd.DataFrame(X_clr, columns=feature_columns, index=X_full.index)\nX_full = X_full.replace([np.inf, -np.inf], np.nan).fillna(0)\nclr_time = time.time() - clr_start\nprint(f\"   CLR transformation completed in {clr_time:.2f}s\")\n\ny = train_features_df[sample_type_col]\n\n# Label encoding \nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\nclass_names = le.classes_\n\nprint(f\"Features: {X_full.shape}\")\nprint(f\"Classes: {list(class_names)}\")\n\n# Feature selection\nprint(\"üìà Feature selection...\")\nfs_start = time.time()\nnon_zero_var_features = X_full.var() > 0\nX_filtered = X_full.loc[:, non_zero_var_features]\n\nn_features_to_select = min(MAX_FEATURES, X_filtered.shape[1])\nselector = SelectKBest(score_func=deterministic_mutual_info, k=n_features_to_select)\nX_selected = selector.fit_transform(X_filtered, y_encoded)\nfs_time = time.time() - fs_start\nprint(f\"   Feature selection completed in {fs_time:.2f}s\")\n\nprint(f\"Selected features: {X_selected.shape}\")\n\n# Scaling \nprint(\"üìè Scaling...\")\nscaler = StandardScaler()\nX_train_final = scaler.fit_transform(X_selected)\n\n# Process test data with SAME pipeline\nprint(\"üß™ Processing test data...\")\ntest_filename_col = 'filename'\ntest_feature_cols = [col for col in test_kmercount_df.columns if col != test_filename_col]\n\n# Create test feature matrix with same columns as training\nX_test_full = pd.DataFrame(0.0, index=test_kmercount_df.index, columns=feature_columns)\n\n# Fill with common features\ncommon_features = set(feature_columns) & set(test_feature_cols)\nfor feature in common_features:\n    X_test_full[feature] = test_kmercount_df[feature].fillna(0)\n\nprint(f\"Test features aligned: {len(common_features)} common features\")\n\n# Apply CLR transformation to test data\nX_test_clr = clr_transform(X_test_full.values)\nX_test_full = pd.DataFrame(X_test_clr, columns=feature_columns, index=X_test_full.index)\nX_test_full = X_test_full.replace([np.inf, -np.inf], np.nan).fillna(0)\n\n# Apply feature selection and scaling\nX_test_filtered = X_test_full.loc[:, non_zero_var_features]\nX_test_selected = selector.transform(X_test_filtered)\nX_test_final = scaler.transform(X_test_selected)\n\nprint(f\"Test data processed: {X_test_final.shape}\")\n\n# Store data dictionary \ndata = {\n    'X_train': X_train_final,\n    'y_train': y_encoded,\n    'X_test': X_test_final,\n    'classes': class_names,\n    'n_classes': len(class_names),\n    'label_encoder': le,\n    'test_ids': test_kmercount_df[test_filename_col].values,\n    'n_features': X_train_final.shape[1]\n}\n\nlog_runtime(\"data_preprocessing\", step_start)\nprint(\"‚úÖ Data preprocessing complete!\")\nprint(f\"   Training data: {data['X_train'].shape}\")\nprint(f\"   Test data: {data['X_test'].shape}\")\nprint(f\"   Classes: {list(data['classes'])}\")\nprint(f\"   Features: {data['n_features']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T14:18:09.738553Z","iopub.execute_input":"2025-09-18T14:18:09.739107Z","iopub.status.idle":"2025-09-18T14:25:49.247630Z","shell.execute_reply.started":"2025-09-18T14:18:09.739086Z","shell.execute_reply":"2025-09-18T14:25:49.246870Z"}},"outputs":[{"name":"stdout","text":"üìä STEP 1: DATA PREPROCESSING PIPELINE\n----------------------------------------\nK-mer counts: (2901, 32897)\nTrain labels: (2901, 4)\nTest k-mer counts: (1068, 32897)\nMerged data: (2901, 32900)\nSample distribution: {'Stool': 811, 'Skin': 787, 'Nasal': 710, 'Mouth': 593}\nüî¨ Applying CLR transformation...\n   CLR transformation completed in 2.82s\nFeatures: (2901, 32896)\nClasses: ['Mouth', 'Nasal', 'Skin', 'Stool']\nüìà Feature selection...\n   Feature selection completed in 377.07s\nSelected features: (2901, 2000)\nüìè Scaling...\nüß™ Processing test data...\nTest features aligned: 32896 common features\nTest data processed: (1068, 2000)\n‚è±Ô∏è  data_preprocessing: 459.50s\n‚úÖ Data preprocessing complete!\n   Training data: (2901, 2000)\n   Test data: (1068, 2000)\n   Classes: ['Mouth', 'Nasal', 'Skin', 'Stool']\n   Features: 2000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 3. Federated Learning Setup","metadata":{}},{"cell_type":"code","source":"class SimpleDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.LongTensor(y) if y is not None else None\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X[idx], self.y[idx]\n        return self.X[idx]\n\ndef create_deterministic_dataloader(dataset, batch_size, shuffle=True):\n    \"\"\"Create DataLoader with deterministic behavior\"\"\"\n    generator = torch.Generator()\n    generator.manual_seed(SEED)\n    \n    return DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=shuffle,\n        generator=generator,\n        worker_init_fn=lambda worker_id: torch.manual_seed(SEED + worker_id),\n        drop_last=False\n    )\n\nclass SimpleMLP(nn.Module):\n    \"\"\"Simple MLP with deterministic initialization\"\"\"\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(DROPOUT),\n            \n            nn.Linear(512, 256),\n            nn.ReLU(), \n            nn.BatchNorm1d(256),\n            nn.Dropout(DROPOUT),\n            \n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Dropout(DROPOUT/2),\n            \n            nn.Linear(128, num_classes)\n        )\n        \n        # Deterministic weight initialization\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        \"\"\"Reset parameters with deterministic initialization\"\"\"\n        torch.manual_seed(SEED)  # Ensure deterministic init\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        return self.network(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T14:30:15.262982Z","iopub.execute_input":"2025-09-18T14:30:15.263267Z","iopub.status.idle":"2025-09-18T14:30:15.272640Z","shell.execute_reply.started":"2025-09-18T14:30:15.263246Z","shell.execute_reply":"2025-09-18T14:30:15.271912Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 4. Federated Training","metadata":{}},{"cell_type":"code","source":"def create_deterministic_federated_splits():\n    \"\"\"Create deterministic federated splits\"\"\"\n    step_start = time.time()\n    print(\"\\nüåê Creating DETERMINISTIC federated splits...\")\n    \n    # Reset random state for consistent splits\n    setup_deterministic_environment(SEED)\n    \n    X_train = data['X_train']\n    y_train = data['y_train']\n    classes = data['classes']\n    \n    client_data = {}\n    \n    # DETERMINISTIC strategy: each client gets all of one class + deterministic samples from others\n    for class_idx, class_name in enumerate(classes):\n        print(f\"   Client {class_name}:\")\n        \n        # Reset random state for each client\n        np.random.seed(SEED + class_idx)\n        \n        # Get all samples of this class\n        class_mask = y_train == class_idx\n        class_X = X_train[class_mask]\n        class_y = y_train[class_mask]\n        \n        # Add some samples from other classes (deterministic selection)\n        n_other_per_class = min(50, len(class_X) // 5)\n        other_X_list = [class_X]\n        other_y_list = [class_y]\n        \n        for other_idx in range(len(classes)):\n            if other_idx != class_idx:\n                other_mask = y_train == other_idx\n                other_X = X_train[other_mask]\n                other_y = y_train[other_mask]\n                \n                if len(other_X) >= n_other_per_class:\n                    # Deterministic selection instead of random\n                    indices = np.arange(len(other_X))\n                    np.random.shuffle(indices)  # Uses seeded random state\n                    selected_indices = indices[:n_other_per_class]\n                    other_X_list.append(other_X[selected_indices])\n                    other_y_list.append(other_y[selected_indices])\n        \n        # Combine\n        client_X = np.vstack(other_X_list)\n        client_y = np.concatenate(other_y_list)\n        \n        # Deterministic shuffle\n        indices = np.arange(len(client_X))\n        np.random.shuffle(indices)  # Uses seeded random state\n        client_X = client_X[indices]\n        client_y = client_y[indices]\n        \n        # Deterministic train/val split\n        val_size = max(20, int(0.15 * len(client_X)))\n        \n        client_data[class_name] = {\n            'X_train': client_X[:-val_size],\n            'y_train': client_y[:-val_size],\n            'X_val': client_X[-val_size:],\n            'y_val': client_y[-val_size:]\n        }\n        \n        train_dist = np.bincount(client_y[:-val_size], minlength=len(classes))\n        print(f\"      Train: {len(client_X)-val_size}, Val: {val_size}\")\n        print(f\"      Distribution: {dict(zip(classes, train_dist))}\")\n    \n    log_runtime(\"federated_splits\", step_start)\n    return client_data\n\nclass DeterministicFederatedClient:\n    def __init__(self, name, client_data, n_features, n_classes):\n        self.name = name\n        self.data = client_data\n        \n        # Model with deterministic initialization\n        self.model = SimpleMLP(n_features, n_classes).to(device)\n        self.criterion = nn.CrossEntropyLoss()\n        \n        # Deterministic data loaders\n        self.train_loader = create_deterministic_dataloader(\n            SimpleDataset(client_data['X_train'], client_data['y_train']),\n            batch_size=BATCH_SIZE, shuffle=True\n        )\n        self.val_loader = create_deterministic_dataloader(\n            SimpleDataset(client_data['X_val'], client_data['y_val']),\n            batch_size=BATCH_SIZE, shuffle=False\n        )\n    \n    def train_local(self, global_weights=None, epochs=5):\n        if global_weights:\n            self.model.load_state_dict(global_weights)\n        \n        # Deterministic optimizer\n        torch.manual_seed(SEED)\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=LR)\n        self.model.train()\n        \n        total_loss = 0\n        total_samples = 0\n        \n        for epoch in range(epochs):\n            for batch_x, batch_y in self.train_loader:\n                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n                \n                optimizer.zero_grad()\n                outputs = self.model(batch_x)\n                loss = self.criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item() * len(batch_x)\n                total_samples += len(batch_x)\n        \n        avg_loss = total_loss / total_samples\n        return self.model.state_dict(), len(self.data['X_train']), avg_loss\n    \n    def evaluate(self):\n        self.model.eval()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for batch_x, batch_y in self.val_loader:\n                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n                outputs = self.model(batch_x)\n                loss = self.criterion(outputs, batch_y)\n                \n                total_loss += loss.item() * len(batch_x)\n                correct += (outputs.argmax(1) == batch_y).sum().item()\n                total += len(batch_x)\n        \n        return total_loss / total, correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T14:30:21.506294Z","iopub.execute_input":"2025-09-18T14:30:21.506550Z","iopub.status.idle":"2025-09-18T14:30:21.520865Z","shell.execute_reply.started":"2025-09-18T14:30:21.506533Z","shell.execute_reply":"2025-09-18T14:30:21.520221Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 5. Federated Training Execution","metadata":{}},{"cell_type":"code","source":"step_start = time.time()\nprint(\"üöÄ STEP 2: FEDERATED LEARNING TRAINING\")\nprint(\"-\" * 40)\n\n# Create deterministic federated splits\nclient_data_dict = create_deterministic_federated_splits()\n\n# Initialize clients with deterministic setup\nclients = {}\nfor name, client_data in client_data_dict.items():\n    clients[name] = DeterministicFederatedClient(name, client_data, data['n_features'], data['n_classes'])\n\nprint(f\"\\nüåê Federated clients ready: {sorted(list(clients.keys()))}\")\n\ndef federated_averaging(client_weights, client_sizes):\n    \"\"\"Deterministic FedAvg\"\"\"\n    total_size = sum(client_sizes)\n    averaged = {}\n    \n    for key in client_weights[0].keys():\n        averaged[key] = sum(\n            w[key] * size / total_size \n            for w, size in zip(client_weights, client_sizes)\n        )\n    \n    return averaged\n\n# Enhanced federated parameters for better performance\nFEDERATED_CONFIG = {\n    'n_rounds': 10,              # Increased rounds\n    'client_epochs': 5,          # Epochs per client per round\n    'early_stopping': True,      # Stop if converged\n    'patience': 5,               # Rounds to wait for improvement\n    'learning_rate_decay': 0.95, # Decay LR each round\n    'verbose': True              # Show detailed progress\n}\n\ndef run_deterministic_federated_training(clients, config=FEDERATED_CONFIG):\n    \"\"\"DETERMINISTIC federated training with consistent client ordering\"\"\"\n    step_start = time.time()\n    print(f\"üöÄ DETERMINISTIC Federated Training ({config['n_rounds']} max rounds)...\")\n\n    # Initialize global model with deterministic weights\n    global_model = SimpleMLP(data['n_features'], data['n_classes']).to(device)\n    global_weights = global_model.state_dict()\n\n    history = []\n    best_loss = float('inf')\n    patience_counter = 0\n    current_lr = LR\n\n    # CRITICAL: Always process clients in same sorted order\n    client_names = sorted(clients.keys())\n    print(f\"   üìã Client order: {client_names}\")\n\n    for round_num in range(1, config['n_rounds'] + 1):\n        print(f\"\\nüìç Round {round_num}/{config['n_rounds']} (LR: {current_lr:.6f})\")\n\n        # Reset random state for each round\n        setup_deterministic_environment(SEED + round_num)\n\n        # Client training with DETERMINISTIC ORDER\n        client_weights = []\n        client_sizes = []\n        round_losses = []\n\n        for name in client_names:  # Use sorted order, not dict iteration\n            client = clients[name]\n            print(f\"   üèãÔ∏è {name}...\", end=\" \")\n\n            weights, size, loss = client.train_local(\n                global_weights,\n                epochs=config['client_epochs']\n            )\n            client_weights.append(weights)\n            client_sizes.append(size)\n            round_losses.append(loss)\n            print(f\"loss={loss:.4f}\")\n\n        # Aggregate\n        print(\"   ‚öñÔ∏è Aggregating...\")\n        global_weights = federated_averaging(client_weights, client_sizes)\n\n        # Update all clients\n        for name in client_names:  # Same order\n            clients[name].model.load_state_dict(global_weights)\n\n        # Global evaluation\n        total_loss = 0\n        total_acc = 0\n        total_samples = 0\n\n        for name in client_names:  # Same order\n            client = clients[name]\n            loss, acc = client.evaluate()\n            samples = len(client.data['X_val'])\n\n            total_loss += loss * samples\n            total_acc += acc * samples\n            total_samples += samples\n\n        global_loss = total_loss / total_samples\n        global_acc = total_acc / total_samples\n\n        # Store history\n        round_info = {\n            'round': round_num,\n            'loss': global_loss,\n            'acc': global_acc,\n            'client_losses': round_losses,\n            'lr': current_lr\n        }\n        history.append(round_info)\n\n        # Progress reporting\n        avg_client_loss = sum(round_losses) / len(round_losses)\n        print(f\"   üåê Global: loss={global_loss:.4f}, acc={global_acc:.4f} ({global_acc*100:.1f}%)\")\n        print(f\"   üìä Clients avg: {avg_client_loss:.4f}\")\n\n        # Check for improvement\n        if global_loss < best_loss:\n            best_loss = global_loss\n            best_weights = global_weights.copy()\n            patience_counter = 0\n            print(f\"   üåü New best loss: {best_loss:.4f}\")\n        else:\n            patience_counter += 1\n\n            # Patience exceeded\n            if patience_counter >= config['patience']:\n                print(f\"   ‚è∞ Early stopping: no improvement for {config['patience']} rounds\")\n                break\n\n        # Learning rate decay\n        if round_num % 5 == 0:  # Every 5 rounds\n            current_lr *= config['learning_rate_decay']\n            print(f\"   üìâ Learning rate decayed to: {current_lr:.6f}\")\n\n    # Load best weights\n    if 'best_weights' in locals():\n        global_model.load_state_dict(best_weights)\n        print(f\"   üìà Loaded best weights (loss: {best_loss:.4f})\")\n\n    log_runtime(\"federated_training\", step_start)\n    \n    print(f\"\\n‚úÖ DETERMINISTIC federated training complete!\")\n    print(f\"   üèÜ Best loss: {best_loss:.4f}\")\n    print(f\"   üìä Rounds completed: {len(history)}\")\n    print(f\"   üéØ Final accuracy: {history[-1]['acc']:.4f} ({history[-1]['acc']*100:.1f}%)\")\n\n    return global_model, history\n\n# Run deterministic federated training\nfederated_model, fed_history = run_deterministic_federated_training(clients)\n\n# Results - SAME VARIABLE NAMES AS CENTRALIZED\nfed_final_loss = fed_history[-1]['loss']\nfed_final_acc = fed_history[-1]['acc']\n\nprint(f\"\\nüìä DETERMINISTIC Federated Results:\")\nprint(f\"   Final loss: {fed_final_loss:.4f}\")\nprint(f\"   Final accuracy: {fed_final_acc:.4f}\")\n\nmodel_to_use = federated_model\nprint(f\"\\nüéØ Using DETERMINISTIC federated model for predictions\")\n\nlog_runtime(\"federated_training_execution\", step_start)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T14:30:27.889229Z","iopub.execute_input":"2025-09-18T14:30:27.890011Z","iopub.status.idle":"2025-09-18T14:30:43.873819Z","shell.execute_reply.started":"2025-09-18T14:30:27.889987Z","shell.execute_reply":"2025-09-18T14:30:43.873253Z"}},"outputs":[{"name":"stdout","text":"üöÄ STEP 2: FEDERATED LEARNING TRAINING\n----------------------------------------\n\nüåê Creating DETERMINISTIC federated splits...\n‚úÖ Full deterministic mode enabled\n   Client Mouth:\n      Train: 632, Val: 111\n      Distribution: {'Mouth': 508, 'Nasal': 41, 'Skin': 39, 'Stool': 44}\n   Client Nasal:\n      Train: 731, Val: 129\n      Distribution: {'Mouth': 43, 'Nasal': 601, 'Skin': 40, 'Stool': 47}\n   Client Skin:\n      Train: 797, Val: 140\n      Distribution: {'Mouth': 47, 'Nasal': 44, 'Skin': 662, 'Stool': 44}\n   Client Stool:\n      Train: 817, Val: 144\n      Distribution: {'Mouth': 44, 'Nasal': 38, 'Skin': 44, 'Stool': 691}\n‚è±Ô∏è  federated_splits: 0.14s\n\nüåê Federated clients ready: ['Mouth', 'Nasal', 'Skin', 'Stool']\nüöÄ DETERMINISTIC Federated Training (10 max rounds)...\n   üìã Client order: ['Mouth', 'Nasal', 'Skin', 'Stool']\n\nüìç Round 1/10 (LR: 0.001000)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.6467\n   üèãÔ∏è Nasal... loss=0.5684\n   üèãÔ∏è Skin... loss=0.8502\n   üèãÔ∏è Stool... loss=0.7253\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.1173, acc=0.9847 (98.5%)\n   üìä Clients avg: 0.6977\n   üåü New best loss: 0.1173\n\nüìç Round 2/10 (LR: 0.001000)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.3002\n   üèãÔ∏è Nasal... loss=0.2236\n   üèãÔ∏è Skin... loss=0.4392\n   üèãÔ∏è Stool... loss=0.2643\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.0720, acc=0.9905 (99.0%)\n   üìä Clients avg: 0.3068\n   üåü New best loss: 0.0720\n\nüìç Round 3/10 (LR: 0.001000)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.1934\n   üèãÔ∏è Nasal... loss=0.2551\n   üèãÔ∏è Skin... loss=0.3805\n   üèãÔ∏è Stool... loss=0.1928\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.0787, acc=0.9885 (98.9%)\n   üìä Clients avg: 0.2555\n\nüìç Round 4/10 (LR: 0.001000)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.2083\n   üèãÔ∏è Nasal... loss=0.1765\n   üèãÔ∏è Skin... loss=0.2066\n   üèãÔ∏è Stool... loss=0.2108\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.0970, acc=0.9866 (98.7%)\n   üìä Clients avg: 0.2006\n\nüìç Round 5/10 (LR: 0.001000)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.1794\n   üèãÔ∏è Nasal... loss=0.1792\n   üèãÔ∏è Skin... loss=0.1945\n   üèãÔ∏è Stool... loss=0.1450\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.0706, acc=0.9885 (98.9%)\n   üìä Clients avg: 0.1745\n   üåü New best loss: 0.0706\n   üìâ Learning rate decayed to: 0.000950\n\nüìç Round 6/10 (LR: 0.000950)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.1754\n   üèãÔ∏è Nasal... loss=0.1588\n   üèãÔ∏è Skin... loss=0.2005\n   üèãÔ∏è Stool... loss=0.1794\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.0381, acc=0.9924 (99.2%)\n   üìä Clients avg: 0.1785\n   üåü New best loss: 0.0381\n\nüìç Round 7/10 (LR: 0.000950)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.1190\n   üèãÔ∏è Nasal... loss=0.1252\n   üèãÔ∏è Skin... loss=0.2106\n   üèãÔ∏è Stool... loss=0.1346\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.0720, acc=0.9905 (99.0%)\n   üìä Clients avg: 0.1474\n\nüìç Round 8/10 (LR: 0.000950)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.1326\n   üèãÔ∏è Nasal... loss=0.1197\n   üèãÔ∏è Skin... loss=0.1092\n   üèãÔ∏è Stool... loss=0.1296\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.0481, acc=0.9924 (99.2%)\n   üìä Clients avg: 0.1228\n\nüìç Round 9/10 (LR: 0.000950)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.1091\n   üèãÔ∏è Nasal... loss=0.0924\n   üèãÔ∏è Skin... loss=0.1188\n   üèãÔ∏è Stool... loss=0.1046\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.0522, acc=0.9905 (99.0%)\n   üìä Clients avg: 0.1062\n\nüìç Round 10/10 (LR: 0.000950)\n‚úÖ Full deterministic mode enabled\n   üèãÔ∏è Mouth... loss=0.1339\n   üèãÔ∏è Nasal... loss=0.1922\n   üèãÔ∏è Skin... loss=0.1252\n   üèãÔ∏è Stool... loss=0.1266\n   ‚öñÔ∏è Aggregating...\n   üåê Global: loss=0.0296, acc=0.9962 (99.6%)\n   üìä Clients avg: 0.1445\n   üåü New best loss: 0.0296\n   üìâ Learning rate decayed to: 0.000902\n   üìà Loaded best weights (loss: 0.0296)\n‚è±Ô∏è  federated_training: 15.40s\n\n‚úÖ DETERMINISTIC federated training complete!\n   üèÜ Best loss: 0.0296\n   üìä Rounds completed: 10\n   üéØ Final accuracy: 0.9962 (99.6%)\n\nüìä DETERMINISTIC Federated Results:\n   Final loss: 0.0296\n   Final accuracy: 0.9962\n\nüéØ Using DETERMINISTIC federated model for predictions\n‚è±Ô∏è  federated_training_execution: 15.97s\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"15.965360879898071"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## 6. Test Data Processing & Predictions","metadata":{}},{"cell_type":"code","source":"step_start = time.time()\nprint(\"üéØ STEP 3: TEST DATA PROCESSING & PREDICTIONS\")\nprint(\"-\" * 40)\n\ndef generate_deterministic_predictions(model):\n    \"\"\"Generate deterministic test predictions\"\"\"\n    print(\"Generating DETERMINISTIC test predictions...\")\n    \n    # Reset random state for deterministic inference\n    setup_deterministic_environment(SEED)\n    \n    # Create test dataset\n    test_dataset = SimpleDataset(data['X_test'])\n    test_loader = create_deterministic_dataloader(test_dataset, BATCH_SIZE, shuffle=False)\n    \n    # Generate predictions\n    model.eval()\n    all_predictions = []\n    \n    with torch.no_grad():\n        for batch_x in test_loader:\n            batch_x = batch_x.to(device)\n            outputs = model(batch_x)\n            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n            all_predictions.append(probs)\n    \n    test_predictions = np.vstack(all_predictions)\n    \n    # Verify prediction quality\n    max_probs = test_predictions.max(axis=1)\n    min_probs = test_predictions.min(axis=1)\n    \n    print(f\"üìä Prediction Quality Analysis:\")\n    print(f\"   Max probability - mean: {max_probs.mean():.3f}, min: {max_probs.min():.3f}\")\n    print(f\"   Min probability - mean: {min_probs.mean():.3f}, max: {min_probs.max():.3f}\")\n    \n    # Check if predictions are confident (max > 0.7, others < 0.15)\n    confident_samples = np.sum(max_probs > 0.7)\n    uniform_samples = np.sum((max_probs < 0.4) & (max_probs > 0.2))  # Uniform-like\n    \n    print(f\"   Confident predictions (max > 0.7): {confident_samples}/{len(max_probs)} ({confident_samples/len(max_probs)*100:.1f}%)\")\n    print(f\"   Uniform-like predictions (0.2-0.4): {uniform_samples}/{len(max_probs)} ({uniform_samples/len(max_probs)*100:.1f}%)\")\n    \n    # Create submission - SAME FORMAT AS CENTRALIZED\n    submission_df = pd.DataFrame({\n        'filename': data['test_ids']\n    })\n    \n    # Add probability columns for each class - SAME AS CENTRALIZED\n    for i, class_name in enumerate(data['classes']):\n        submission_df[class_name] = test_predictions[:, i]\n    \n    # Save submission file - SAME NAMING AS CENTRALIZED\n    output_file = f\"submission_federated_logloss{fed_final_loss:.4f}.csv\"\n    submission_df.to_csv(output_file, index=False)\n    \n    print(f\"‚úÖ Submission saved: {output_file}\")\n    print(\"First 5 predictions:\")\n    print(submission_df.head())\n    \n    print(f\"\\nüìä Prediction Statistics:\")\n    print(f\"   Total test samples: {len(test_predictions)}\")\n    print(f\"   Prediction shape: {test_predictions.shape}\")\n    print(f\"   Class names: {list(data['classes'])}\")\n    \n    # Warning if predictions look uniform\n    if uniform_samples > len(max_probs) * 0.5:\n        print(f\"\\n   ‚ö†Ô∏è WARNING: Many predictions look uniform! Model may not be learning properly.\")\n        print(f\"      Consider: more training, different architecture, or data issues\")\n    elif confident_samples > len(max_probs) * 0.7:\n        print(f\"\\n   üéâ EXCELLENT: Most predictions are confident!\")\n    else:\n        print(f\"\\n   ‚úÖ GOOD: Reasonable prediction confidence\")\n    \n    return submission_df, output_file\n\n# Generate final deterministic predictions\nfinal_submission, output_filename = generate_deterministic_predictions(model_to_use)\n\nlog_runtime(\"prediction_generation\", step_start)\nprint(\"\\n‚úÖ Test data processing & predictions complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T14:30:55.846264Z","iopub.execute_input":"2025-09-18T14:30:55.846725Z","iopub.status.idle":"2025-09-18T14:30:55.927212Z","shell.execute_reply.started":"2025-09-18T14:30:55.846701Z","shell.execute_reply":"2025-09-18T14:30:55.926556Z"}},"outputs":[{"name":"stdout","text":"üéØ STEP 3: TEST DATA PROCESSING & PREDICTIONS\n----------------------------------------\nGenerating DETERMINISTIC test predictions...\n‚úÖ Full deterministic mode enabled\nüìä Prediction Quality Analysis:\n   Max probability - mean: 0.999, min: 0.534\n   Min probability - mean: 0.000, max: 0.000\n   Confident predictions (max > 0.7): 1067/1068 (99.9%)\n   Uniform-like predictions (0.2-0.4): 0/1068 (0.0%)\n‚úÖ Submission saved: submission_federated_logloss0.0296.csv\nFirst 5 predictions:\n    filename         Mouth         Nasal          Skin         Stool\n0  ID_ABHFUP  2.238452e-18  1.000000e+00  6.444092e-21  6.888429e-09\n1  ID_ADBLNY  7.651327e-20  1.000000e+00  8.019523e-22  1.023180e-10\n2  ID_AFAEMB  9.999987e-01  1.075555e-14  1.339720e-06  1.376729e-19\n3  ID_AFBBWK  1.000000e+00  3.751782e-16  1.540580e-09  4.850357e-22\n4  ID_AGHEZK  1.000000e+00  2.585032e-14  3.140381e-08  1.276789e-19\n\nüìä Prediction Statistics:\n   Total test samples: 1068\n   Prediction shape: (1068, 4)\n   Class names: ['Mouth', 'Nasal', 'Skin', 'Stool']\n\n   üéâ EXCELLENT: Most predictions are confident!\n‚è±Ô∏è  prediction_generation: 0.07s\n\n‚úÖ Test data processing & predictions complete!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 7. Final Results and Runtime Summary","metadata":{}},{"cell_type":"code","source":"total_time = time.time() - start_time_total\nruntime_log['total_pipeline'] = total_time\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üéâ FEDERATED LEARNING PIPELINE COMPLETED SUCCESSFULLY!\")\nprint(\"=\" * 60)\n\nprint(f\"üìä Final Results:\")\nprint(f\"   Federated Loss: {fed_final_loss:.4f}\")\nprint(f\"   Federated Accuracy: {fed_final_acc:.4f} ({fed_final_acc*100:.1f}%)\")\n\nprint(f\"\\n‚è±Ô∏è  Detailed Runtime Summary:\")\nprint(f\"   {'Step':<30} {'Time (s)':<12} {'Time (min)':<12}\")\nprint(\"-\" * 54)\n\nfor step, runtime in runtime_log.items():\n    print(f\"   {step:<30} {runtime:<12.1f} {runtime/60:<12.1f}\")\n\nprint(f\"\\nüìÅ Output files:\")\nprint(f\"   Federated submission: {output_filename}\")\n\nprint(f\"\\nüéØ Key Deterministic Features Applied:\")\nprint(f\"   ‚úÖ Complete random state control (Python/NumPy/PyTorch)\")\nprint(f\"   ‚úÖ Deterministic DataLoader with generators\")\nprint(f\"   ‚úÖ Fixed model weight initialization\")\nprint(f\"   ‚úÖ Consistent client ordering in federated rounds\")\nprint(f\"   ‚úÖ Deterministic mutual information feature selection\")\nprint(f\"   ‚úÖ Seeded data splits and shuffling\")\n\nprint(f\"\\nüåê Federated Learning Features:\")\nprint(f\"   ‚úÖ 4 federated clients (one per body site)\")\nprint(f\"   ‚úÖ FedAvg aggregation algorithm\")\nprint(f\"   ‚úÖ Deterministic client training order\")\nprint(f\"   ‚úÖ Neural network models per client\")\n\nprint(f\"\\nüïí Total pipeline runtime: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\nprint(f\"üìÖ Completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T14:31:36.923072Z","iopub.execute_input":"2025-09-18T14:31:36.923351Z","iopub.status.idle":"2025-09-18T14:31:36.930863Z","shell.execute_reply.started":"2025-09-18T14:31:36.923331Z","shell.execute_reply":"2025-09-18T14:31:36.930094Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüéâ FEDERATED LEARNING PIPELINE COMPLETED SUCCESSFULLY!\n============================================================\nüìä Final Results:\n   Federated Loss: 0.0296\n   Federated Accuracy: 0.9962 (99.6%)\n\n‚è±Ô∏è  Detailed Runtime Summary:\n   Step                           Time (s)     Time (min)  \n------------------------------------------------------\n   data_preprocessing             459.5        7.7         \n   federated_splits               0.1          0.0         \n   federated_training             15.4         0.3         \n   federated_training_execution   16.0         0.3         \n   prediction_generation          0.1          0.0         \n   total_pipeline                 852.7        14.2        \n\nüìÅ Output files:\n   Federated submission: submission_federated_logloss0.0296.csv\n\nüéØ Key Deterministic Features Applied:\n   ‚úÖ Complete random state control (Python/NumPy/PyTorch)\n   ‚úÖ Deterministic DataLoader with generators\n   ‚úÖ Fixed model weight initialization\n   ‚úÖ Consistent client ordering in federated rounds\n   ‚úÖ Deterministic mutual information feature selection\n   ‚úÖ Seeded data splits and shuffling\n\nüåê Federated Learning Features:\n   ‚úÖ 4 federated clients (one per body site)\n   ‚úÖ FedAvg aggregation algorithm\n   ‚úÖ Deterministic client training order\n   ‚úÖ Neural network models per client\n\nüïí Total pipeline runtime: 852.7s (14.2 minutes)\nüìÖ Completed at: 2025-09-18 14:31:36\n","output_type":"stream"}],"execution_count":9}]}